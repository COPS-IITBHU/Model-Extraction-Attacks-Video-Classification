{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vivit_model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ju4hygPlXJl"
      },
      "outputs": [],
      "source": [
        "!pip3 install --user torch\n",
        "!pip3 install --user torchvision\n",
        "!pip3 install --user matplotlib\n",
        "!pip3 install --user decord\n",
        "!pip3 install --user einops\n",
        "!pip3 install --user scikit-image\n",
        "!pip3 install --user pytorch-lightning\n",
        "from torch.utils.data import DataLoader\n",
        "!pip3 install gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ViViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV0rNgKCqvxN",
        "outputId": "e4d77bdd-91f6-4116-97f5-c25a7a1a36a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1--whzasg232ISfR9O_yWAgPwih9utYiL"
      ],
      "metadata": {
        "id": "_h0Ba2sEpOUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch==1.8.0+cu101 torchvision==0.9.0+cu101 torchtext==0.9.0 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.8.0/index.html\n",
        "\n",
        "# Install mmaction2\n",
        "!rm -rf mmaction2\n",
        "!git clone https://github.com/open-mmlab/mmaction2.git\n",
        "%cd mmaction2\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "773G6lAPl42U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ViViT"
      ],
      "metadata": {
        "id": "7nOMU4q-sx2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install einops\n",
        "import einops\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from module import Attention, PreNorm, FeedForward\n",
        "import numpy as np\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "  \n",
        "class ViViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, num_frames, dim = 192, depth = 4, heads = 3, pool = 'cls', in_channels = 3, dim_head = 64, dropout = 0.,\n",
        "                 emb_dropout = 0., scale_dim = 4, ):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "            \n",
        "        # model = ViViT(224, 16, 400, 16).cuda()\n",
        "\n",
        "\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = in_channels * patch_size ** 2\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_frames, num_patches + 1, dim))\n",
        "        self.space_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.space_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)\n",
        "\n",
        "        self.temporal_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.temporal_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.pool = pool\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.to_patch_embedding(x)\n",
        "        b, t, n, _ = x.shape\n",
        "\n",
        "        cls_space_tokens = repeat(self.space_token, '() n d -> b t n d', b = b, t=t)\n",
        "        x = torch.cat((cls_space_tokens, x), dim=2)\n",
        "        x += self.pos_embedding[:, :, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = rearrange(x, 'b t n d -> (b t) n d')\n",
        "        x = self.space_transformer(x)\n",
        "        x = rearrange(x[:, 0], '(b t) ... -> b t ...', b=b)\n",
        "\n",
        "        cls_temporal_tokens = repeat(self.temporal_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_temporal_tokens, x), dim=1)\n",
        "\n",
        "        x = self.temporal_transformer(x)\n",
        "        \n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        return self.mlp_head(x)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "npyEJm9jl9kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install mmaction2\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "5nXzkQ2-mEQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import mmaction2\n",
        "sys.path.insert(1, './ViViT/')\n",
        "from models.swint_victim import SwinTransformer3D as VICTIM\n",
        "#from models.swint_student import SwinTransformer3D as STUDENT\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "USE_CUDA = True\n",
        "PRETRAINED = True\n",
        "TRAIN = True\n",
        "epochs = 5\n",
        "from cv2 import transform\n",
        "import torchvision\n",
        "import cv2\n",
        "#import network\n",
        "# from approximate_gradients import *\n",
        "# from cifar10_models import *\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def measure_true_grad_norm(args, x):\n",
        "    # Compute true gradient of loss wrt x\n",
        "    true_grad, _ = compute_gradient(\n",
        "        args, args.teacher, args.student, x, pre_x=True, device=args.device)\n",
        "\n",
        "    # PROGRESS UNTIL 13TH MARCH 1:57 AM IST\n",
        "\n",
        "    true_grad = true_grad.reshape(-1, args.batch_size * 196608)  # 3145728\n",
        "\n",
        "    # Compute norm of gradients\n",
        "    norm_grad = true_grad.norm(2, dim=1).mean().cpu()\n",
        "    return norm_grad\n",
        "\n",
        "transform_swint = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(224),\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[123.675, 116.28, 103.53],\n",
        "        std=[58.395, 57.12, 57.375]\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('result400.csv')\n",
        "ll = sorted(list(df.iloc[:,1].unique()))\n",
        "dic = {}\n",
        "for id, i in enumerate(ll):\n",
        "    dic[i] = id\n",
        "\n",
        "# dic\n",
        "\n",
        "# (24 * x) / 16\n",
        "# 10 * 10s 100 (5 frames)\n",
        "\n",
        "def video2img(video_path, transform=transform_swint):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    success, image = vidcap.read()\n",
        "    l = []\n",
        "    fc = 0\n",
        "    frems = []\n",
        "    while success:\n",
        "        frems.append(image)\n",
        "        success, image = vidcap.read()\n",
        "    fc = len(frems) \n",
        "    \n",
        "    # print(fc)\n",
        "    \n",
        "    if fc == 0:\n",
        "        print(video_path)\n",
        "        \n",
        "    for i in range(0, fc, (fc-1)//15):\n",
        "        image = frems[i]\n",
        "        # print(image.shape)\n",
        "        l.append(\n",
        "            transform(\n",
        "                torch.tensor(image).type(\n",
        "                    torch.FloatTensor).permute(2, 0, 1)\n",
        "            )\n",
        "        )\n",
        "    if len(l) == 0:\n",
        "        torch.zeros(3, 16, 224, 224)\n",
        "    else:\n",
        "        return torch.stack(l[:16], dim=1)\n",
        "\n",
        "\n",
        "class videosDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, root_dir, transform=transform_swint):\n",
        "        \n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        vid_path = os.path.join(self.root_dir, (self.annotations.iloc[index, 0]))\n",
        "        # print(vid_path)\n",
        "        vid_label = torch.tensor(dic[self.annotations.iloc[index, 1]])\n",
        "        # put the labels into a dictionary?\n",
        "        \n",
        "        vid = video2img(vid_path, self.transform)\n",
        "        \n",
        "        # if self.transform:\n",
        "            # vid = self.transform(vid)\n",
        "        \n",
        "        return (vid, vid_label)\n",
        "        \n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "rlqkDNGBmJFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ViViT"
      ],
      "metadata": {
        "id": "LT1vCXvXtuoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs = 5\n",
        "gen_lr = 1e-4\n",
        "dis_lr = 1e-4\n",
        "device = 'cuda'\n",
        "csv_file = 'result400.csv'\n",
        "root_dir = './'\n",
        "rr = './'\n",
        "data = videosDataset(csv_file, rr, transform = transform_swint)\n",
        "\n",
        "train_loader = DataLoader(dataset = data, batch_size = batch_size, shuffle = True)\n",
        "os.listdir(rr)\n",
        "# for data, targets in (train_loader):"
      ],
      "metadata": {
        "id": "y0Mk3iICmW0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   print(data)\n",
        "\n",
        "def use_pretrained(model,\n",
        "                   folder='weights/',\n",
        "                   file_name=\"swint_victim_pretrained.pth\",\n",
        "                   download=False,\n",
        "                   url=None, ):\n",
        "    if download:\n",
        "        response = requests.get(url, stream=True)\n",
        "        t = int(response.headers.get('content-length', 0))  # total file size\n",
        "        block_size = 1024 ** 2  # 1 Mbit\n",
        "        progress_bar = tqdm(total=t, unit='iB', unit_scale=True)\n",
        "        with open(f\"weights/{file_name}\", 'wb') as file:\n",
        "            for data in response.iter_content(block_size):\n",
        "                progress_bar.update(len(data))\n",
        "                file.write(data)\n",
        "        progress_bar.close()\n",
        "        if (t != 0) and (progress_bar.n != t):\n",
        "            print(\"ERROR downloading weights!\")\n",
        "            return -1\n",
        "        print(f\"Weights downloaded in {folder} directory!\")\n",
        "    model.load_state_dict(torch.load(os.path.join(folder, file_name)))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "#student_model = STUDENT()\n",
        "victim_model = VICTIM()\n",
        "if PRETRAINED:\n",
        "    victim_model = use_pretrained(victim_model)\n",
        "if USE_CUDA:\n",
        "    #student_model.cuda()\n",
        "    victim_model.cuda()\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    #student_model.parameters(),\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=0.02\n",
        ")\n",
        "\n",
        "victim_model.eval()"
      ],
      "metadata": {
        "id": "dtuSwLy2mcqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "model = ViViT(224, 16, 400, 16).cuda()\n",
        "for epoch in range(epochs):\n",
        "    \"\"\"\n",
        "    This will run until query is being generated by the generator or else generator \\\n",
        "    will change TRAIN to False.\n",
        "    \"\"\"\n",
        "    # Converting video to image of size [BatchSize=1, channels=3, frames, height=224, width=224]\n",
        "    # image = video2img(VIDEO_PATH)\n",
        "    #image = video2img(data_path)\n",
        "\n",
        "    for image, targets in (train_loader):\n",
        "        \n",
        "        # try:\n",
        "          \n",
        "          # if __name__ == \"__main__\":\n",
        "        img = image.permute(0, 2, 1, 3, 4)\n",
        "        img = img.cuda()\n",
        "        #img = torch.ones([1, 13, 3, 224, 224]).cuda()\n",
        "\n",
        "        # parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        # parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "        # print('Trainable Parameters: %.3fM' % parameters)\n",
        "        out = model(img)\n",
        "          #out = torch.nn.functional.softmax(out)\n",
        "\n",
        "            # print(\"Shape of out :\", out.shape)      # [B, num_classes]\n",
        "          #if USE_CUDA:\n",
        "        image = image.cuda()\n",
        "        #image = torch.ones([1, 3, 16, 224, 224]).cuda()\n",
        "        # Querying the label from victim model\n",
        "        with torch.no_grad():\n",
        "            LABEL = torch.argmax(victim_model(image), 1).cuda()\n",
        "        #target = torch.zeros(1, 400)  # (BatchSize, Classes)\n",
        "        #target[:, LABEL] = 1\n",
        "\n",
        "        # LABEL = LABEL\n",
        "        # Training the student model\n",
        "        #output = student_model(image)\n",
        "        loss = loss_func(out, LABEL)\n",
        "        # print(loss)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss)\n",
        "        \n",
        "    print('loss at epoch %d =', epoch, sum(losses)/(len(losses))\n",
        "         )\n"
      ],
      "metadata": {
        "id": "FWC5I4cdmjq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/ViViT.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ex2CBsm9vBIr",
        "outputId": "3068b1f6-ba0c-48f1-8cb8-5b09b1c39ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4b192b26-39cf-420a-b38f-4394cd15bdea\", \"ViViT.zip\", 189962770)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}